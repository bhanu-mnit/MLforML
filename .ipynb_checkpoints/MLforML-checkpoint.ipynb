{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random;\n",
    "import scipy \n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "from pandas import ExcelWriter\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('predictors.csv');\n",
    "responses = pd.read_csv('responses.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Stratification | Feature Selection | Data Replication | Model Building | Classification & Regression\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply Regression\n",
    "def build_models(predictors, responses, modelNo):\n",
    "    if(modelNo==0):\n",
    "        # Linear Regression\n",
    "        model = linear_model.LinearRegression();\n",
    "        modelName = \"Linear Regression\";\n",
    "    if(modelNo==1):\n",
    "        # Ridge Regression\n",
    "        model = linear_model.RidgeCV(alphas = (0.1,0.1,10));\n",
    "        modelName = \"Ridge Regression\";\n",
    "    if(modelNo==2):\n",
    "        # lasso Regression\n",
    "        model = linear_model.MultiTaskLassoCV(eps=0.001, n_alphas=100, alphas=(0.1,0.1,10));\n",
    "        modelName = \"Lasso Regression\";\n",
    "    model.fit(predictors, responses);\n",
    "    predictions = model.predict(predictors);\n",
    "    Result = {};\n",
    "    Result['modelName'] = modelName;\n",
    "    Result['predictions'] = predictions;\n",
    "    Result['model'] = model;\n",
    "    Result['Corr'] = pearsonr(predictions,responses)[0][0];\n",
    "    return Result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result = []; \n",
    "for i in range(0,3):\n",
    "    temp = build_models(predictors, responses, 0);\n",
    "    Result.append(temp[''])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(selectData):\n",
    "    if(selectData == 0):\n",
    "        features = feat_matrix[0,0];\n",
    "        labels = labels_matrix[0,0];\n",
    "    if(selectData == 2):\n",
    "        features = feat_matrix[0,2];\n",
    "        labels = labels_matrix[0,2];\n",
    "    if(selectData == 4):\n",
    "        features = feat_matrix[0,4];\n",
    "        labels = labels_matrix[0,4];\n",
    "    features = pd.DataFrame(features);\n",
    "    labels = pd.DataFrame(labels);\n",
    "    lab = labels[labels[0]!= -1.0]\n",
    "    feat = features[labels[0]!= -1.0]\n",
    "    feat = pd.DataFrame(feat.values);\n",
    "    return lab, feat;\n",
    "\n",
    "def get_data_whole(selectData):\n",
    "    features = pd.concat([pd.DataFrame(feat_matrix[0,selectData]),pd.DataFrame(feat_matrix_sty[0,selectData])],axis=1);\n",
    "    labels = labels_matrix[0,selectData];\n",
    "    labels = pd.DataFrame(labels);\n",
    "    lab = labels[labels[0]!= -1.0]\n",
    "    feat = features[labels[0]!= -1.0]\n",
    "    feat = pd.DataFrame(feat.values);\n",
    "    lab = pd.DataFrame(lab.values);\n",
    "    keys_mat = test['feat_keys'][0][selectData];\n",
    "    keys_sty = test['feat_keys_sty'][0];\n",
    "    final_keys = np.append(keys_mat, keys_sty);\n",
    "    return lab, feat, final_keys;\n",
    "\n",
    "def apply_Model(temp_data, selectModel):\n",
    "    data = {};\n",
    "    data['X_train_ceil'] = temp_data['X_train_ceil'];\n",
    "    data['X_test_ceil'] = temp_data['X_test_ceil'];\n",
    "    data['y_train_ceil'] = temp_data['y_train_ceil'];\n",
    "    data['y_test_ceil'] = temp_data['y_test_ceil'];\n",
    "    data['ind_ceil'] = temp_data['ind_ceil'] \n",
    "    \n",
    "    # feature selection for the floor\n",
    "    data['X_train_floor'] = temp_data['X_train_floor'];\n",
    "    data['X_test_floor'] = temp_data['X_test_floor'];\n",
    "    data['y_train_floor'] = temp_data['y_train_floor'] ;\n",
    "    data['y_test_floor'] = temp_data['y_test_floor'];\n",
    "    data['ind_floor'] = temp_data['ind_floor'];\n",
    "    \n",
    "    if(selectModel==1):\n",
    "        print \"OneVsRest\";\n",
    "        classifier_floor = OneVsRestClassifier(LinearSVC(random_state=0)).fit(data['X_train_floor'], data['y_train_floor'])\n",
    "        classifier_ceil = OneVsRestClassifier(LinearSVC(random_state=0)).fit(data['X_train_ceil'], data['y_train_ceil'])\n",
    "    if(selectModel==2):\n",
    "        print \"Decision Tree\";\n",
    "        classifier_floor = tree.DecisionTreeClassifier().fit(data['X_train_floor'], data['y_train_floor'])\n",
    "        classifier_ceil = tree.DecisionTreeClassifier().fit(data['X_train_ceil'], data['y_train_ceil'])\n",
    "    if(selectModel==3):\n",
    "        print \"Nearest Centroid\";\n",
    "        classifier_floor = NearestCentroid().fit(data['X_train_floor'], np.ravel(data['y_train_floor']));\n",
    "        classifier_ceil = NearestCentroid().fit(data['X_train_ceil'], np.ravel(data['y_train_ceil']));\n",
    "    if(selectModel==4):\n",
    "        print \"SGD Classifier\";\n",
    "        classifier_floor = SGDClassifier(loss=\"hinge\", penalty=\"l2\").fit(data['X_train_floor'], np.ravel(data['y_train_floor']));\n",
    "        classifier_ceil = SGDClassifier(loss=\"hinge\", penalty=\"l2\").fit(data['X_train_ceil'], np.ravel(data['y_train_ceil']));\n",
    "    \n",
    "    train_predict_floor = classifier_floor.predict(data['X_train_floor']);\n",
    "    conf_mat_floor = confusion_matrix(train_predict_floor,data['y_train_floor']);\n",
    "    train_predict_ceil = classifier_ceil.predict(data['X_train_ceil']);\n",
    "    conf_mat_ceil = confusion_matrix(train_predict_ceil, data['y_train_ceil'])\n",
    "    \n",
    "    y_predict_ceil = classifier_ceil.predict(data['X_test_ceil'])\n",
    "    result_ceil = confusion_matrix(y_predict_ceil,data['y_test_ceil'])\n",
    "    \n",
    "    y_predict_floor = classifier_floor.predict(data['X_test_floor'])\n",
    "    result_floor = confusion_matrix(y_predict_floor,data['y_test_floor'])\n",
    "    \n",
    "    precision_floor, recall_floor, _, _ = precision_recall_fscore_support(data['y_test_floor'], y_predict_floor)\n",
    "    precision_ceil, recall_ceil, _, _ = precision_recall_fscore_support(data['y_test_ceil'], y_predict_ceil)\n",
    "    \n",
    "    acc_ceil = result_ceil.trace()*100/result_ceil.sum();\n",
    "    acc_ceil_train = conf_mat_ceil.trace()*100/conf_mat_ceil.sum();\n",
    "    acc_floor = result_floor.trace()*100/result_floor.sum();\n",
    "    acc_floor_train = conf_mat_floor.trace()*100/conf_mat_floor.sum();\n",
    "    \n",
    "    data['acc_ceil_train'] = acc_ceil_train;\n",
    "    data['acc_floor_train'] = acc_floor_train;\n",
    "    \n",
    "    return data, acc_ceil, acc_floor, recall_ceil, recall_floor, result_ceil, result_floor, conf_mat_floor, conf_mat_ceil;\n",
    "\n",
    "def apply_regression_model(X_train, y_train, X_test, y_test, indices, selectModel):\n",
    "    Result = {};\n",
    "    Result['X_train'] = X_train;\n",
    "    Result['y_train'] = y_train; \n",
    "    Result['X_test'] = X_test;\n",
    "    Result['y_test'] = y_test;\n",
    "    Result['indices'] = indices;\n",
    "    if(selectModel==0):\n",
    "        print \"Linear Regression\";\n",
    "        model = linear_model.LinearRegression();\n",
    "        model.fit(X_train, y_train);\n",
    "        predictions = model.predict(X_test);\n",
    "        predictions_train = model.predict(X_train);\n",
    "    if(selectModel==1):\n",
    "        print \"Ridge Regression\";\n",
    "        model = linear_model.RidgeCV(alphas = (0.1,0.1,10));\n",
    "        model.fit(X_train, y_train);\n",
    "        predictions = model.predict(X_test);\n",
    "        predictions_train = model.predict(X_train);\n",
    "    if(selectModel==2):\n",
    "        print \"Lasso Regression\";\n",
    "        model = linear_model.MultiTaskLassoCV(eps=0.001, n_alphas=100, alphas=(0.1,0.1,10));\n",
    "        model.fit(X_train, y_train);\n",
    "        predictions = model.predict(X_test);\n",
    "        predictions_train = model.predict(X_train);\n",
    "    Result['predictions'] = predictions;\n",
    "    Result['model'] = model;\n",
    "    Result['predictions_train'] = predictions_train;\n",
    "    return Result;\n",
    "\n",
    "def get_data_allModel(lab, feat, selectFeatTech):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feat, lab, test_size=0.3, random_state=42);\n",
    "    X_train, y_train = replicate_data(X_train, y_train);\n",
    "    X_test, y_test = replicate_data(X_test, y_test);\n",
    "    X_train, X_test , indices = feature_selection_regression(X_train, y_train, X_test, selectFeatTech);\n",
    "    return X_train, y_train, X_test, y_test, indices;\n",
    "\n",
    "def get_data_allModel_class(lab, feat, selectFeatTech):\n",
    "    lab_ceil, lab_floor = get_lab_ceil(lab);\n",
    "    data = {};\n",
    "    chk = True;\n",
    "    # just to make sure that there is no case where only 2 classes are there in training or testing set\n",
    "    while(chk):\n",
    "        X_train_ceil, X_test_ceil, y_train_ceil, y_test_ceil = train_test_split(feat, lab_ceil, test_size=0.3, random_state=10);\n",
    "        if(len(pd.unique(y_train_ceil[0].ravel()))==3 & len(pd.unique(y_test_ceil[0].ravel()))==3):\n",
    "            chk = False;\n",
    "    data['X_train_before'] = X_train_ceil;\n",
    "    data['y_train_before'] = y_train_ceil;\n",
    "    data['X_test_before'] = X_test_ceil;\n",
    "    data['y_test_before'] = y_test_ceil;\n",
    "    X_train_ceil, y_train_ceil = replicate_data(X_train_ceil, y_train_ceil);\n",
    "    X_test_ceil, y_test_ceil = replicate_data(X_test_ceil, y_test_ceil);\n",
    "    data['X_train_after'] = X_train_ceil;\n",
    "    data['y_train_after'] = y_train_ceil;\n",
    "    data['X_test_after'] = X_test_ceil;\n",
    "    data['y_test_after'] = y_test_ceil;\n",
    "    \n",
    "    # feature Selection for the ceil    \n",
    "    X_train_ceil, X_test_ceil, indices_ceil = feature_selection_tech(X_train_ceil,y_train_ceil[0],X_test_ceil,selectFeatTech)\n",
    "    data['X_train_ceil'] = X_train_ceil;\n",
    "    data['X_test_ceil'] = X_test_ceil;\n",
    "    data['y_train_ceil'] = y_train_ceil;\n",
    "    data['y_test_ceil'] = y_test_ceil;\n",
    "    data['ind_ceil'] = indices_ceil;\n",
    "    \n",
    "    # just to make sure that there is no case where only 2 classes are there in training or testing set\n",
    "    chk = True;\n",
    "    while(chk):\n",
    "        X_train_floor, X_test_floor, y_train_floor, y_test_floor = train_test_split(feat, lab_floor, test_size=0.3, random_state=10)\n",
    "        if(len(pd.unique(y_train_floor[0].ravel()))==3 & len(pd.unique(y_test_floor[0].ravel()))==3):\n",
    "            chk = False;\n",
    "    X_train_floor, y_train_floor = replicate_data(X_train_floor, y_train_floor);\n",
    "    X_test_floor, y_test_floor = replicate_data(X_test_floor, y_test_floor);\n",
    "    \n",
    "    # feature selection for the floor\n",
    "    X_train_floor, X_test_floor, indices_floor = feature_selection_tech(X_train_floor,y_train_floor[0],X_test_floor,selectFeatTech)\n",
    "    data['X_train_floor'] = X_train_floor;\n",
    "    data['X_test_floor'] = X_test_floor;\n",
    "    data['y_train_floor'] = y_train_floor;\n",
    "    data['y_test_floor'] = y_test_floor;\n",
    "    data['ind_floor'] = indices_floor;\n",
    "    return data;\n",
    "\n",
    "def get_lab_ceil(lab):\n",
    "    lab_ceil = np.ceil(lab);\n",
    "    lab_floor = np.floor(lab);\n",
    "    return lab_ceil, lab_floor;\n",
    "\n",
    "def replicate_data(x_val, y_val):\n",
    "    ind1 = y_val[y_val[0]==1].shape[0];\n",
    "    ind2 = y_val[y_val[0]==2].shape[0];\n",
    "    ind3 = y_val[y_val[0]==3].shape[0];\n",
    "    max_val = max(ind1,ind2,ind3);\n",
    "    # we are increasing all the data to the 90% of the maximum of the class that is present.\n",
    "    cnt = int(max_val*1);\n",
    "    if(ind1>0 & ind1!=max_val):\n",
    "        new_cnt = cnt-ind1;\n",
    "        pred = x_val[y_val[0]==1];\n",
    "        resp = y_val[y_val[0]==1];\n",
    "        for x in range(0,new_cnt):\n",
    "            pos = np.random.randint(pred.shape[0]);\n",
    "            val = pred.iloc[[pos]];\n",
    "            res = resp.iloc[[pos]];\n",
    "            x_val = pd.concat([x_val,val]);\n",
    "            y_val = pd.concat([y_val,res]);\n",
    "    \n",
    "    if(ind2>0 & ind2!=max_val):\n",
    "        new_cnt = cnt-ind2;\n",
    "        pred = x_val[y_val[0]==2];\n",
    "        resp = y_val[y_val[0]==2];\n",
    "        for x in range(0,new_cnt):\n",
    "            pos = np.random.randint(pred.shape[0]);\n",
    "            val = pred.iloc[[pos]];\n",
    "            res = resp.iloc[[pos]];\n",
    "            x_val = pd.concat([x_val,val]);\n",
    "            y_val = pd.concat([y_val,res]);    \n",
    "    \n",
    "    if(ind3>0 & ind3!=max_val):\n",
    "        new_cnt = cnt-ind3;\n",
    "        pred = x_val[y_val[0]==3];\n",
    "        resp = y_val[y_val[0]==3];\n",
    "        for x in range(0,new_cnt):\n",
    "            pos = np.random.randint(pred.shape[0]);\n",
    "            val = pred.iloc[[pos]];\n",
    "            res = resp.iloc[[pos]];\n",
    "            x_val = pd.concat([x_val,val]);\n",
    "            y_val = pd.concat([y_val,res]);\n",
    "    return x_val, y_val;\n",
    "\n",
    "# feature selection techniques\n",
    "#X_train_floor, X_test_floor, indices = feature_selection_tech(X_train_floor,y_train_floor[0],X_test_floor,selectFeatTech)\n",
    "def feature_selection_tech(predictors, responses, test_predictors, selectFeatTech):\n",
    "    if(selectFeatTech==0):\n",
    "        t=int(predictors.shape[1]*0.40);\n",
    "        t=40;\n",
    "        model = SelectKBest(chi2, k=t).fit(predictors, responses);\n",
    "        predictors_new = model.transform(predictors);\n",
    "        predictors_test_new = model.transform(test_predictors);\n",
    "        indices = model.get_support(indices=True);\n",
    "    if(selectFeatTech==1):\n",
    "        randomized_logistic = RandomizedLogisticRegression();\n",
    "        model = randomized_logistic.fit(predictors, responses);\n",
    "        predictors_new = model.transform(predictors);\n",
    "        predictors_test_new = model.transform(test_predictors);\n",
    "        indices = model.get_support(indices=True);\n",
    "    return predictors_new, predictors_test_new, indices;\n",
    "\n",
    "def feature_selection_regression(predictors, responses, test_predictors, selectFeatTech):\n",
    "    if(selectFeatTech==0):        \n",
    "        chk = int(predictors.shape[1]*0.40);\n",
    "        # have fixed the value of how many features are to be selected as of now.\n",
    "        model = SelectKBest(f_regression, k=25);\n",
    "        model = model.fit(predictors, responses[0]);\n",
    "        predictors_new = model.transform(predictors);\n",
    "        predictors_test_new = model.transform(test_predictors);\n",
    "        indices = model.get_support(indices=True);\n",
    "        print \"SelectKBest -> \"+str(len(indices));\n",
    "    if(selectFeatTech==1):\n",
    "        model = RandomizedLasso(alpha='aic', scaling=0.3, sample_fraction=0.60, n_resampling=200, selection_threshold=0.15);\n",
    "        model = model.fit(predictors, responses[0]);\n",
    "        predictors_new = model.transform(predictors);\n",
    "        predictors_test_new = model.transform(test_predictors);\n",
    "        indices = model.get_support(indices=True);\n",
    "        print \"Randomized Lasso -> \"+str(len(indices));\n",
    "    return predictors_new, predictors_test_new, indices;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression Functions. The next cell contains their iterative versions.\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classification_results():\n",
    "    # Final Cell to get all the classification results.\n",
    "    presentFeatTech = 0;\n",
    "    Result_dump = [];\n",
    "    features_selected_dump = [];\n",
    "    cnt = 0;\n",
    "    feat_tech = ['SelectK','Randomized Lasso'];\n",
    "    mod_tech = ['OneVsRest','Decision Trees','Nearest Centroid','SGD Classifier'];\n",
    "    index_dict = {};\n",
    "    for dataSet in range(0,5,2):\n",
    "        lab, feat, final_keys = get_data_whole(dataSet)\n",
    "        # Selecting data once for all of the models.\n",
    "        for presentFeatTech in range(0,2):\n",
    "            temp_data = get_data_allModel_class(lab, feat, presentFeatTech);\n",
    "            for modelNo in range(1,4):\n",
    "                index_dict[cnt] = mod_tech[modelNo-1];\n",
    "                cnt = cnt+1;\n",
    "                data0, acc_ceil, acc_floor, recall_ceil, recall_floor, result_ceil, result_floor, conf_mat_floor, conf_mat_ceil = apply_Model(temp_data,modelNo);\n",
    "                sel_features_floor = final_keys[data0['ind_floor']];\n",
    "                sel_features_floor = [x[0] for x in sel_features_floor];\n",
    "                sel_features_ceil = final_keys[data0['ind_ceil']];\n",
    "                sel_features_ceil = [str(x[0]) for x in sel_features_ceil];\n",
    "                Result_dump.append([data0['acc_ceil_train'],acc_ceil, conf_mat_ceil,result_ceil,len(sel_features_ceil),sel_features_ceil,\n",
    "                                    data0['acc_floor_train'],acc_floor, conf_mat_floor, result_floor,len(sel_features_floor),sel_features_floor]);\n",
    "    A = pd.DataFrame(Result_dump)\n",
    "    A.rename(columns={0:'Train-Acc-Ceil',1:'Test-Acc-Ceil',2:'Train-Conf-Ceil',3:'Test-Conf-Ceil',4:'FeatNo-Ceil',5:'Selected Features-Ceil',\n",
    "                  6:'Train-Acc-Floor',7:'Test-Acc-Floor',8:'Train-Conf-Floor',9:'Test-Conf-Floor',10:'FeatNo-Floor',11:'Selected Features-Floor'}, inplace=True)\n",
    "    A.rename(index=index_dict, inplace=True)\n",
    "    return A;\n",
    "\n",
    "def get_regression_results():\n",
    "    # Final Regression result dumpings.\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # For all of the models and all of the feature selection techniques\n",
    "    Result = {};\n",
    "    final_stats = [];\n",
    "    mod_tech = ['linear','Ridge','Lasso'];\n",
    "    mod_dist = {};\n",
    "    cnt = 0;\n",
    "    for dataSelect in range(0,5,2):\n",
    "        lab, feat, final_keys = get_data_whole(dataSelect)\n",
    "        print \"\\n\"\n",
    "        print 'Java-sID->'+str(test['sids'][0][dataSelect])\n",
    "        Result[dataSelect] = {};\n",
    "        Result[dataSelect]['final_keys'] = final_keys;\n",
    "        for presentFeatRegress in range(0,2):\n",
    "            Result[dataSelect][presentFeatRegress] = {};\n",
    "            # Just to select one data for all of the models. So that the result of models are comparable\n",
    "            X_train, y_train, X_test, y_test, indices = get_data_allModel(lab, feat, presentFeatRegress);\n",
    "            for modelNo in range(0,3):\n",
    "                Result[dataSelect][presentFeatRegress][modelNo] = apply_regression_model(X_train, y_train, X_test, y_test, indices, modelNo);\n",
    "                present_data = Result[dataSelect][presentFeatRegress][modelNo];\n",
    "                Corr_train = pearsonr(Result[dataSelect][presentFeatRegress][modelNo]['y_train'].values, Result[dataSelect][presentFeatRegress][modelNo]['predictions_train'])[0];\n",
    "                Corr_test = pearsonr(Result[dataSelect][presentFeatRegress][modelNo]['y_test'].values, Result[dataSelect][presentFeatRegress][modelNo]['predictions'])[0];\n",
    "                selected_feat = final_keys[present_data['indices']];\n",
    "                selected_feat = [str(x[0]) for x in selected_feat];\n",
    "                final_stats.append([Corr_train[0], Corr_test[0],len(selected_feat),selected_feat]);\n",
    "                mod_dist[cnt] = mod_tech[modelNo];\n",
    "                cnt = cnt+1;\n",
    "    A = pd.DataFrame(final_stats);\n",
    "    A.rename(columns={0:'Corr-Train',1:'Corr-Test',2:'FeatNo',3:'Features Selected'},inplace=True);\n",
    "    A.rename(index=mod_dist, inplace=True)\n",
    "    return A;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classification_iterate_results(iterate_times):\n",
    "    for i in range(0,iterate_times):\n",
    "        if(i==0):\n",
    "            val = get_classification_results();\n",
    "            val = val[['Train-Acc-Ceil','Test-Acc-Ceil','FeatNo-Ceil','Train-Conf-Ceil','Test-Conf-Ceil','Train-Acc-Floor','Test-Acc-Floor','FeatNo-Floor','Train-Conf-Floor','Test-Conf-Floor']];\n",
    "        else:\n",
    "            temp_val = get_classification_results();\n",
    "            temp_val = temp_val[['Train-Acc-Ceil','Test-Acc-Ceil','FeatNo-Ceil','Train-Conf-Ceil','Test-Conf-Ceil','Train-Acc-Floor','Test-Acc-Floor','FeatNo-Floor','Train-Conf-Floor','Test-Conf-Floor']];\n",
    "            val = val + temp_val;\n",
    "    val = val/iterate_times;\n",
    "    return val;\n",
    "\n",
    "def get_regression_iterate_results(iterate_times):\n",
    "    for i in range(0,iterate_times):\n",
    "        if(i==0):\n",
    "            val = get_regression_results();\n",
    "            val = val[['Corr-Train','Corr-Test','FeatNo']];\n",
    "        else:\n",
    "            temp_val = get_regression_results();\n",
    "            temp_val = temp_val[['Corr-Train','Corr-Test','FeatNo']];\n",
    "            val = val+temp_val;\n",
    "    val = val/iterate_times;\n",
    "    return val;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Result_class = get_classification_iterate_results(10);\n",
    "#Result_regression = get_regression_iterate_results(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n",
      "OneVsRest\n",
      "Decision Tree\n",
      "Nearest Centroid\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    iterate_items = 10;\n",
    "    if(iterate_items):\n",
    "        Result_class = get_classification_iterate_results(iterate_items);\n",
    "        #Result_regression = get_regression_iterate_results(iterate_items);\n",
    "        Result_class.to_csv(\"Result_class_iterate10.csv\");\n",
    "        #Result_regression.to_csv(\"Result_regress_iterate10.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Train-Acc-Ceil', u'Test-Acc-Ceil', u'Train-Conf-Ceil', u'Test-Conf-Ceil', u'FeatNo-Ceil', u'Selected Features-Ceil', u'Train-Acc-Floor', u'Test-Acc-Floor', u'Train-Conf-Floor', u'Test-Conf-Floor', u'FeatNo-Floor', u'Selected Features-Floor'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
